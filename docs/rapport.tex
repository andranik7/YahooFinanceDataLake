\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, fit, backgrounds}

\geometry{margin=2.5cm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    citecolor=black
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{Projet Data Lake}
\lhead{Yahoo Finance Data Lake}
\rfoot{Page \thepage}

\title{
    \vspace{-1cm}
    \textbf{Projet Data Lake} \\[0.3cm]
    \Large Yahoo Finance  Sentiment Analysis \\[0.5cm]
    \large Andranick HAYRAPETYAN | Antoine LEFEVRE
}
\author{}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\vfill
\setcounter{tocdepth}{1}
\tableofcontents
\newpage

%% ================================================================
\section{Introduction}
%% ================================================================

Ce projet met en place une architecture Big Data complète pour l'analyse de données boursières. Il couvre l'ensemble du cycle de vie de la donnée, depuis l'ingestion de sources externes jusqu'à la prédiction de cours via des modèles de séries temporelles, en passant par la transformation, l'enrichissement et la visualisation.

L'architecture repose sur un Data Lake organisé en couches successives, orchestré par Apache Airflow, et exposé au travers de dashboards Kibana. L'ensemble des services est conteneurisé via Docker Compose, garantissant une reproductibilité totale du déploiement.

Le projet suit dix symboles boursiers du marché américain : Apple (AAPL), Google (GOOGL), Microsoft (MSFT), Amazon (AMZN), Meta (META), Tesla (TSLA), NVIDIA (NVDA), JPMorgan (JPM), Visa (V) et Walmart (WMT). Ces valeurs couvrent plusieurs secteurs (technologie, finance, grande distribution) et offrent une diversité représentative du marché.

%% ================================================================
\section{Architecture générale}
%% ================================================================

\subsection{Vue d'ensemble}

L'architecture s'articule autour de cinq phases distinctes formant un pipeline de bout en bout. Les données brutes sont d'abord collectées depuis Yahoo Finance et l'API Finnhub, puis normalisées en format Parquet via Apache Spark. Une étape d'enrichissement croise les différentes sources, avant qu'un modèle SARIMAX ne génère des prédictions intégrant le sentiment des actualités. Enfin, l'ensemble est indexé dans Elasticsearch et visualisé dans Kibana.

\subsection{Organisation du Data Lake}

Le Data Lake est structuré en trois couches principales, chacune répondant à un objectif précis dans le cycle de traitement des données.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}llX@{}}
\toprule
Couche & Format & Description \\
\midrule
Raw & JSON & Données brutes telles que reçues des APIs, partitionnées par date d'ingestion. Aucune transformation n'est appliquée à ce stade. \\
Formatted & Parquet & Données normalisées avec validation des types, conversion des timestamps en UTC et compression Snappy. \\
Usage & Parquet & Données enrichies résultant des jointures entre sources (cours, entreprises, actualités) et des métriques dérivées. \\
\bottomrule
\end{tabularx}
\caption{Couches du Data Lake}
\end{table}

Les chemins de stockage suivent une convention stricte. Les données brutes Yahoo Finance sont stockées dans \texttt{data/raw/yahoo\_finance/}, les actualités dans \texttt{data/raw/news/}. Après transformation, les fichiers Parquet sont organisés dans \texttt{data/formatted/} puis \texttt{data/usage/}. Les prédictions sont écrites dans \texttt{data/usage/predictions/}.

\subsection{Stack technique}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lX@{}}
\toprule
Technologie & Rôle \\
\midrule
Apache Airflow & Orchestration du pipeline \\
Apache Spark & Transformation distribuée (normalisation, jointures) \\
statsmodels (SARIMAX) & Prédiction de séries temporelles avec variables exogènes \\
VADER & Analyse de sentiment des actualités \\
Elasticsearch + Kibana & Indexation, recherche et visualisation \\
PostgreSQL & Métadonnées Airflow \\
Docker Compose & Conteneurisation et déploiement \\
\bottomrule
\end{tabularx}
\caption{Stack technique du projet (versions détaillées dans le tableau~\ref{tab:docker})}
\end{table}

%% ================================================================
\section{Infrastructure Docker}
%% ================================================================

L'ensemble de l'infrastructure est déployé via Docker Compose. Six services collaborent sur un réseau bridge dédié (\texttt{bigdata-network}).

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lllX@{}}
\toprule
Service & Version & Port & Rôle \\
\midrule
Airflow & 2.7.3 & 8080 & Orchestration du pipeline, interface web d'administration \\
PostgreSQL & 15 & 5432 & Base de métadonnées Airflow (LocalExecutor) \\
Spark Master & 3.5.0 & 7077 / 8081 & Coordinateur du cluster Spark \\
Spark Worker & 3.5.0 & -- & Nœud de calcul (2 Go de mémoire alloués) \\
Elasticsearch & 8.11.0 & 9200 & Indexation et recherche (512 Mo -- 1 Go de heap) \\
Kibana & 8.11.0 & 5601 & Interface de visualisation connectée à Elasticsearch \\
\bottomrule
\end{tabularx}
\caption{Services Docker déployés}
\label{tab:docker}
\end{table}

L'image Airflow est construite à partir d'un Dockerfile personnalisé (\texttt{Dockerfile.airflow}) basé sur \texttt{apache/airflow:2.7.3-python3.10}. Cette image intègre OpenJDK 11 pour l'exécution de Spark, avec une détection dynamique de l'architecture processeur (ARM64 ou AMD64) assurant la compatibilité sur différentes plateformes. Les dépendances Python installées incluent \texttt{yfinance}, \texttt{pandas}, \texttt{pyarrow}, \texttt{pyspark}, \texttt{elasticsearch}, \texttt{vaderSentiment} et \texttt{statsmodels}.

Les volumes Docker montent les répertoires du projet (DAGs, scripts, données, configuration) dans le conteneur Airflow, permettant un développement itératif sans reconstruction de l'image. Le volume \texttt{elasticsearch-data} assure la persistance des index entre les redémarrages.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{docker.png}

\caption{Services Docker en cours d'exécution}
\end{figure}

%% ================================================================
\section{Sources de données}
%% ================================================================

\subsection{Cours boursiers -- Yahoo Finance}

La bibliothèque \texttt{yfinance} permet de récupérer l'historique des cours sur cinq années pour chaque symbole. Les données incluent les prix d'ouverture, de clôture, les extremums journaliers et le volume échangé. En complément, les informations statiques de chaque entreprise (nom, secteur, industrie, capitalisation boursière) sont collectées.

L'historique de cinq ans représente environ 1\,260 jours de trading par symbole, soit un total d'environ 12\,600 enregistrements pour les dix symboles suivis. Les données sont sauvegardées en JSON, partitionnées par date d'ingestion.

\subsection{Actualités financières -- Finnhub}

L'API Finnhub fournit les actualités financières associées à chaque symbole. L'ingestion s'effectue mois par mois afin de contourner les limites de pagination de l'API : pour chaque symbole, douze appels sont effectués (un par mois), soit 120 appels au total. Un délai de 1,1 seconde entre chaque requête respecte la limite de 60 appels par minute du tier gratuit.

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
Paramètre & Valeur \\
\midrule
Période couverte & 12 mois \\
Symboles suivis & 10 \\
Appels API totaux & 120 \\
Délai entre appels & 1,1 seconde \\
Durée totale d'ingestion & environ 3 minutes \\
Volume collecté & environ 21\,000 articles \\
\bottomrule
\end{tabular}
\caption{Paramètres d'ingestion des actualités Finnhub}
\end{table}

Chaque article est dédupliqué via son identifiant UUID fourni par l'API. Les champs collectés incluent le titre, le résumé, l'éditeur, la catégorie, la date de publication et l'URL.

%% ================================================================
\section{Analyse de sentiment}
%% ================================================================

L'analyse de sentiment est réalisée lors de l'ingestion des actualités à l'aide de VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER est un analyseur lexical conçu pour les textes courts, particulièrement adapté aux titres et résumés d'articles financiers.

Pour chaque article, le texte du titre et du résumé est concaténé puis soumis à l'analyseur. VADER produit un score composite compris entre $-1.0$ (très négatif) et $+1.0$ (très positif). Ce score est ensuite classifié selon les seuils suivants :

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Label & Condition \\
\midrule
Positif & score $\geq 0.05$ \\
Négatif & score $\leq -0.05$ \\
Neutre & $-0.05 <$ score $< 0.05$ \\
\bottomrule
\end{tabular}
\caption{Seuils de classification du sentiment VADER}
\end{table}

Le score et le label sont stockés avec chaque article dans la couche Raw puis propagés jusqu'à Elasticsearch, où ils sont disponibles pour la visualisation et, surtout, pour alimenter le modèle de prédiction en tant que variable exogène.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{news.png}
\caption{Actualités financières avec analyse de sentiment dans Kibana}
\end{figure}

%% ================================================================
\section{Transformation et enrichissement}
%% ================================================================

\subsection{Normalisation (Formatting)}

La première étape de transformation convertit les fichiers JSON bruts en format Parquet via Apache Spark. Le traitement inclut le casting des types numériques (Double pour les prix, Long pour les volumes), la conversion des timestamps en UTC et la validation du schéma. Le format Parquet, avec sa structure colonnaire et sa compression Snappy, offre des performances de lecture supérieures au JSON pour les opérations analytiques.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{spark.png}

\caption{Spark UI -- Jobs de transformation exécutés}
\end{figure}

\subsection{Enrichissement (Combination)}

L'enrichissement croise les trois sources de données par une série de jointures gauches sur le champ \texttt{symbol}. Les cours boursiers sont d'abord joints avec les informations d'entreprise (nom, secteur, industrie, capitalisation), puis avec une agrégation des actualités par symbole (nombre total d'articles et date du dernier article).

Deux métriques dérivées sont calculées à cette étape. L'amplitude journalière (\texttt{daily\_range}) correspond à la différence entre le prix le plus haut et le plus bas de la journée. La variation journalière (\texttt{daily\_change\_pct}) mesure l'écart entre le prix de clôture et d'ouverture, exprimé en pourcentage.

Le dataset enrichi final est écrit dans \texttt{data/usage/stock\_analysis/enriched\_stocks.parquet} et constitue la base pour la phase de prédiction et l'indexation.

%% ================================================================
\section{Prédiction des cours}
%% ================================================================

\subsection{Approche retenue}

Le projet intègre une phase de machine learning pour la prédiction des cours financiers. Le modèle retenu est SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous variables), qui combine l'analyse de séries temporelles avec une variable externe : le sentiment des actualités financières. Ce choix permet de croiser deux dimensions complémentaires -- la dynamique historique des prix et le contexte médiatique -- pour produire des prédictions plus contextualisées.

\subsection{Modèle SARIMAX}

Le modèle SARIMAX étend le modèle ARIMA classique en ajoutant une composante saisonnière et la possibilité d'intégrer des variables exogènes. La formulation mathématique s'exprime comme suit :

\begin{equation}
\phi(B)\Phi(B^s)(1-B)^d(1-B^s)^D y_t = \theta(B)\Theta(B^s)\epsilon_t + \beta x_t
\end{equation}

où $B$ est l'opérateur de retard, $s$ la période saisonnière, $\phi$ et $\Phi$ les polynômes autorégressifs, $\theta$ et $\Theta$ les polynômes de moyenne mobile, $d$ et $D$ les ordres de différenciation, $x_t$ la variable exogène (sentiment) et $\beta$ son coefficient.

\subsection{Paramétrage}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lrX@{}}
\toprule
Paramètre & Valeur & Signification \\
\midrule
Ordre ARIMA & $(2, 1, 2)$ & 2 termes autorégressifs, 1 différenciation, 2 termes de moyenne mobile \\
Ordre saisonnier & $(1, 1, 1, 5)$ & Composante saisonnière hebdomadaire (5 jours ouvrés) \\
Variable exogène & Sentiment & Score de sentiment agrégé quotidien par symbole \\
Période d'entraînement & 252 jours & Dernière année de trading (environ 252 jours ouvrés) \\
Horizon de prédiction & 30 jours & 30 jours ouvrés de prédiction \\
Intervalle de confiance & 95\% & Bandes de confiance à $\alpha = 0.05$ \\
Itérations maximales & 200 & Limite d'optimisation pour la convergence du modèle \\
\bottomrule
\end{tabularx}
\caption{Paramètres du modèle SARIMAX}
\end{table}

\subsection{Intégration du sentiment}

Les scores de sentiment VADER (décrits en section~5) sont agrégés en moyenne quotidienne par symbole et fournis au modèle comme variable exogène.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{kibana1.png}
\caption{Cours réels et prédictions SARIMAX avec bandes de confiance à 95\%}
\end{figure}

\subsection{Sortie du modèle}

Le script produit un fichier Parquet contenant les 90 derniers jours de données réelles (continuité visuelle) et les 30 jours de prédictions. Le schéma de sortie correspond au mapping de l'index \texttt{stock\_predictions} décrit en section~\ref{sec:predictions-index}.

%% ================================================================
\section{Indexation Elasticsearch}
%% ================================================================

Les données sont exposées via trois index Elasticsearch, chacun répondant à un besoin spécifique de consultation et de visualisation.

\subsection{Index stock\_analysis}

Cet index contient les données boursières enrichies. Chaque document correspond à un jour de trading pour un symbole donné, identifié par la clé \texttt{symbol\_date}. L'indexation est incrémentale : les nouvelles exécutions du pipeline mettent à jour les documents existants sans supprimer l'historique.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}llX@{}}
\toprule
Champ & Type & Description \\
\midrule
symbol & keyword & Symbole boursier \\
name & text & Nom de l'entreprise \\
sector, industry & keyword & Secteur et industrie \\
date & date & Date du cours \\
open, high, low, close & float & Prix d'ouverture, plus haut, plus bas, clôture \\
volume & long & Volume échangé \\
market\_cap & long & Capitalisation boursière \\
daily\_range & float & Amplitude journalière (high $-$ low) \\
daily\_change\_pct & float & Variation journalière en pourcentage \\
news\_count & integer & Nombre d'articles associés au symbole \\
latest\_news\_date & date & Date du dernier article \\
\bottomrule
\end{tabularx}
\caption{Mapping de l'index stock\_analysis (environ 12\,800 documents)}
\end{table}

\subsection{Index stock\_news}

Cet index stocke les actualités financières avec leur analyse de sentiment. La recherche plein texte est activée sur les champs titre et résumé, permettant des requêtes par mots-clés depuis Kibana. L'identifiant de chaque document est l'UUID fourni par l'API Finnhub, garantissant la déduplication.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}llX@{}}
\toprule
Champ & Type & Description \\
\midrule
symbol & keyword & Symbole associé \\
title & text & Titre de l'article (recherche plein texte) \\
summary & text & Résumé de l'article (recherche plein texte) \\
provider & keyword & Éditeur de l'article \\
category & keyword & Catégorie (company, market, etc.) \\
pub\_date\_utc & date & Date de publication en UTC \\
sentiment\_score & float & Score de sentiment VADER ($-1.0$ à $+1.0$) \\
sentiment\_label & keyword & Classification (positive, negative, neutral) \\
\bottomrule
\end{tabularx}
\caption{Mapping de l'index stock\_news (environ 21\,000 documents)}
\end{table}

\subsection{Index stock\_predictions}
\label{sec:predictions-index}

Cet index contient les prédictions SARIMAX et les données historiques récentes servant de référence visuelle. Contrairement aux deux autres index, celui-ci est recréé intégralement à chaque exécution du pipeline, les prédictions devant être recalculées à chaque mise à jour des données.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}llX@{}}
\toprule
Champ & Type & Description \\
\midrule
symbol & keyword & Symbole boursier \\
date & date & Date de la valeur \\
predicted\_close & float & Cours prédit ou réel \\
confidence\_lower & float & Borne inférieure de l'intervalle de confiance \\
confidence\_upper & float & Borne supérieure de l'intervalle de confiance \\
sentiment\_score & float & Score de sentiment utilisé pour la prédiction \\
type & keyword & \texttt{actual} ou \texttt{forecast} \\
\bottomrule
\end{tabularx}
\caption{Mapping de l'index stock\_predictions (environ 1\,200 documents)}
\end{table}

%% ================================================================
\section{Orchestration Airflow}
%% ================================================================

L'ensemble du pipeline est orchestré par un DAG (Directed Acyclic Graph) Airflow planifié pour une exécution quotidienne. Le DAG enchaîne six tâches, certaines pouvant s'exécuter en parallèle.

\begin{figure}[H]
\centering
\fbox{\parbox{0.95\textwidth}{
\centering
\vspace{0.3cm}
\texttt{start} $\rightarrow$ \texttt{[ingest\_stocks $\|$ ingest\_news]} $\rightarrow$ \texttt{format\_data} $\rightarrow$ \texttt{combine\_data} $\rightarrow$ \texttt{predict\_arima} $\rightarrow$ \texttt{index\_data} $\rightarrow$ \texttt{end}
\vspace{0.3cm}
}}
\caption{Structure du DAG \texttt{yahoo\_finance\_pipeline}}
\end{figure}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}llX@{}}
\toprule
Tâche & Opérateur & Description \\
\midrule
ingest\_stocks & PythonOperator & Collecte des cours sur 5 ans et des informations d'entreprise via yfinance \\
ingest\_news & PythonOperator & Collecte de 12 mois d'actualités via Finnhub avec analyse de sentiment VADER \\
format\_data & BashOperator & Soumission Spark pour la conversion JSON vers Parquet \\
combine\_data & BashOperator & Soumission Spark pour les jointures et le calcul des métriques \\
predict\_arima & PythonOperator & Exécution du modèle SARIMAX avec sentiment pour chaque symbole \\
index\_data & PythonOperator & Indexation bulk dans les trois index Elasticsearch \\
\bottomrule
\end{tabularx}
\caption{Tâches du DAG Airflow}
\end{table}

Les tâches d'ingestion s'exécutent en parallèle car elles sont indépendantes. Chaque tâche dispose d'une tentative de réessai après un délai de 5 minutes en cas d'échec.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{airflow1.png}
\caption{DAG \texttt{yahoo\_finance\_pipeline} dans l'interface Airflow}
\end{figure}


%% ================================================================
\section{Visualisation Kibana}
%% ================================================================

Les dashboards Kibana sont automatiquement importés au démarrage via le script \texttt{init\_kibana.sh}, qui charge le fichier \texttt{kibana/kibana\_saved\_objects.ndjson} via l'API REST. Ce fichier est versionné dans Git, assurant la reproductibilité des visualisations.

L'application comprend deux dashboards complémentaires : un dashboard principal offrant une vue d'ensemble de tous les symboles (évolution des cours, prédictions, top/flop, capitalisation par secteur, sentiment), et un dashboard détaillé accessible par drill-down permettant l'analyse approfondie d'un symbole individuel (métriques, cours, prédiction, rendement, corrélation avec le sentiment, actualités).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{dashboard1.png}
\caption{Dashboard principal — Seconde partie}
\end{figure}

\begin{figure}[H]                                                                                                  
  \centering                                                                                                         
  \includegraphics[width=\textwidth, height=1\textheight, keepaspectratio]{dashboard2.png}                        
  \caption{Dashboard détaillé — Détail d'un cours}                                                                   
\end{figure}  

%% ================================================================
\section{Volumétrie et performance}
%% ================================================================

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
Étape & Durée approximative \\
\midrule
Ingestion des cours & 30 secondes \\
Ingestion des actualités & 3 minutes \\
Normalisation Spark & 1 minute \\
Enrichissement Spark & 1 minute \\
Prédiction SARIMAX & 30 secondes \\
Indexation Elasticsearch & 1 minute \\
\midrule
Pipeline complet & 6 à 7 minutes \\
\bottomrule
\end{tabular}
\caption{Temps d'exécution du pipeline}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{index_m.png}
\caption{Index Elasticsearch après exécution du pipeline}
\end{figure}

%% ================================================================
\section{Conclusion}
%% ================================================================

Ce projet démontre la faisabilité d'un pipeline analytique de bout en bout sur des données financières, du Data Lake à la prédiction. Le croisement entre séries temporelles (SARIMAX) et traitement du langage naturel (sentiment VADER) illustre l'intérêt de combiner des approches complémentaires pour contextualiser les prédictions.

L'ensemble est conteneurisé et déployable en une commande, rendant le projet reproductible sur tout environnement disposant de Docker.

Un article détaillé présentant l'architecture et les choix techniques du projet a été publié sur Medium : \url{https://medium.com/@andranikhayra/building-a-financial-data-lake-with-sentiment-powered-stock-predictions-1d331b199973}

\end{document}
